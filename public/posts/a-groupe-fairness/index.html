<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - http://localhost:1313/">
    <title>A Groupe Fairness | Bloggin on Responsible AI</title>
    <meta name="description" content="Bloggin on Responsible AI">
    <meta property="og:url" content="http://localhost:1313/posts/a-groupe-fairness/">
  <meta property="og:site_name" content="Bloggin on Responsible AI">
  <meta property="og:title" content="A Groupe Fairness">
  <meta property="og:description" content="FRAPPÉ: Making AI Fair Without Retraining Introduction Fairness in predictive models has become a crucial issue in the rapidly developing field of machine learning. Reducing bias in these systems is crucial to preventing the continuation of social injustices as machine learning algorithms are increasingly used in high-stakes applications including criminal justice, lending, and employment.
Consider creating a machine learning model to predict student outcomes, loan approvals, or job performance. On the whole, the model performs well, but on closer inspection, you find that it treats several demographic groups unfairly. It is often necessary to retrain the entire model to correct this bias in typical machine learning pipelines, which can be costly, time-consuming, or even impossible if you don’t have access to the learning pipeline.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-01T22:29:01+01:00">
    <meta property="article:modified_time" content="2025-03-01T22:29:01+01:00">

    
  <meta itemprop="name" content="A Groupe Fairness">
  <meta itemprop="description" content="FRAPPÉ: Making AI Fair Without Retraining Introduction Fairness in predictive models has become a crucial issue in the rapidly developing field of machine learning. Reducing bias in these systems is crucial to preventing the continuation of social injustices as machine learning algorithms are increasingly used in high-stakes applications including criminal justice, lending, and employment.
Consider creating a machine learning model to predict student outcomes, loan approvals, or job performance. On the whole, the model performs well, but on closer inspection, you find that it treats several demographic groups unfairly. It is often necessary to retrain the entire model to correct this bias in typical machine learning pipelines, which can be costly, time-consuming, or even impossible if you don’t have access to the learning pipeline.">
  <meta itemprop="datePublished" content="2025-03-01T22:29:01+01:00">
  <meta itemprop="dateModified" content="2025-03-01T22:29:01+01:00">
  <meta itemprop="wordCount" content="1264">
    
    <link rel="canonical" href="http://localhost:1313/posts/a-groupe-fairness/">
    <link rel="icon" href="http://localhost:1313//assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Bloggin on Responsible AI" href="http://localhost:1313//atom.xml" />
    <link rel="alternate" type="application/json" title="Bloggin on Responsible AI" href="http://localhost:1313//feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%} </style>
  
    
  
  
  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      "articleSection": "posts",
      "name": "A Groupe Fairness",
      "headline": "A Groupe Fairness",
      "alternativeHeadline": "",
      "description": "\u003ch1 id=\u0022frappé-making-ai-fair-without-retraining\u0022\u003eFRAPPÉ: Making AI Fair Without Retraining\u003c\/h1\u003e\n\u003ch2 id=\u0022introduction\u0022\u003eIntroduction\u003c\/h2\u003e\n\u003cp\u003eFairness in predictive models has become a crucial issue in the rapidly developing field of machine learning. Reducing bias in these systems is crucial to preventing the continuation of social injustices as machine learning algorithms are increasingly used in high-stakes applications including criminal justice, lending, and employment.\u003cbr\u003e\nConsider creating a machine learning model to predict student outcomes, loan approvals, or job performance. On the whole, the model performs well, but on closer inspection, you find that it treats several demographic groups unfairly. It is often necessary to retrain the entire model to correct this bias in typical machine learning pipelines, which can be costly, time-consuming, or even impossible if you don\u0026rsquo;t have access to the learning pipeline.\u003c\/p\u003e",
      "inLanguage": "en-us",
      "isFamilyFriendly": "true",
      "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "http:\/\/localhost:1313\/posts\/a-groupe-fairness\/"
      },
      "author" : {
          "@type": "Person",
          "name": ""
      },
      "creator" : {
          "@type": "Person",
          "name": ""
      },
      "accountablePerson" : {
          "@type": "Person",
          "name": ""
      },
      "copyrightHolder" : "Bloggin on Responsible AI",
      "copyrightYear" : "2025",
      "dateCreated": "2025-03-01T22:29:01.00Z",
      "datePublished": "2025-03-01T22:29:01.00Z",
      "dateModified": "2025-03-01T22:29:01.00Z",
      "publisher":{
          "@type":"Organization",
          "name": "Bloggin on Responsible AI",
          "url": "http://localhost:1313/",
          "logo": {
              "@type": "ImageObject",
              "url": "http:\/\/localhost:1313\/assets\/favicon.ico",
              "width":"32",
              "height":"32"
          }
      },
      "image": "http://localhost:1313/assets/favicon.ico",
      "url" : "http:\/\/localhost:1313\/posts\/a-groupe-fairness\/",
      "wordCount" : "1264",
      "genre" : [ ],
      "keywords" : [ ]
  }
  </script>
  
  
  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    <header>
<p style="padding: 0;margin: 0;">
  <a href="../../">
    <b>Bloggin on Responsible AI</b>
    <span class="text-stone-500 animate-blink">▮</span>
  </a>
</p>
<ul style="padding: 0;margin: 0;">
  
  
  <li class="">
    <a href="../../posts/"><span>Post</span></a>
    
  <li class="">
    <a href="../../tutorial/"><span>Tutorial</span></a>
    
  <li class="">
    <a href="../../about/"><span>About</span></a>
    
  <li class="">
    <a href="../../articles/"><span>Articles</span></a>
    
  </li>
</ul>
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post">A Groupe Fairness</h2>
      <h1 id="frappé-making-ai-fair-without-retraining">FRAPPÉ: Making AI Fair Without Retraining</h1>
<h2 id="introduction">Introduction</h2>
<p>Fairness in predictive models has become a crucial issue in the rapidly developing field of machine learning. Reducing bias in these systems is crucial to preventing the continuation of social injustices as machine learning algorithms are increasingly used in high-stakes applications including criminal justice, lending, and employment.<br>
Consider creating a machine learning model to predict student outcomes, loan approvals, or job performance. On the whole, the model performs well, but on closer inspection, you find that it treats several demographic groups unfairly. It is often necessary to retrain the entire model to correct this bias in typical machine learning pipelines, which can be costly, time-consuming, or even impossible if you don&rsquo;t have access to the learning pipeline.</p>
<h2 id="frappé-is-here">FRAPPÉ is here</h2>
<p>FRAPPÉ (Fairness Framework for Post-Processing Everything) offers a revolutionary method and is building the roadmap to the way we think about fairness in machine learning. In this blog, we’ll try to go through the key ideas, contributions, and implications of this innovative work.</p>
<h2 id="why-fairness-in-ai-matters">Why Fairness in AI Matters</h2>
<p>(AI) algorithms are increasingly  affecting people&rsquo;s lives, from loan applications to job decisions to medical diagnoses.  If there&rsquo;s a chance that these processes can be compromised by bias, they are likely to promote systemic discrimination.</p>
<p>Consequently, one of the key issues in the ethical development of AI is <strong>group fairness</strong>, i.e. ensuring that different demographic groups (e.g. based on race, gender or age) receive fair treatment.
Ensuring that algorithms do not disproportionately harm or help certain groups, particularly those identified by sensitive characteristics such as age, gender or race, is the goal of fairness in artificial intelligence.  Achieving fairness, however, is no simple matter.  Each of the thousands of definitions of fairness (such as equal opportunity, statistical parity and equalization of opportunity) involves trade-offs between predictive accuracy and fairness.</p>
<p>Traditionally, there are three types of fairness mitigation strategies:</p>
<ul>
<li><strong>Pre-processing</strong>: Removing bias from training data.</li>
<li><strong>In-processing</strong>: Incorporating fairness constraints or regularizers during training.</li>
<li><strong>Post-processing</strong>: Adjusting the outputs to achieve fairness.</li>
</ul>
<p>On one hand, in-processing are highly effective ,they often require retraining of the entire model, which is known to be computationally expensive and impractical in many real-life situations.  On the other hand, post-processing offers more flexibility, it has been limited to particular contexts and fairness standards.</p>
<hr>
<h2 id="why-frappé-is-a-game-changer-">Why FRAPPÉ is a Game-Changer ?</h2>
<h3 id="1-works-with-any-model">1. Works with Any Model</h3>
<p>FRAPPÉ is <strong>model-agnostic</strong> — meaning thay it can is well functional with deep neural networks, decision trees, or even black-box models . We can put it as the following if your model produces scores or logits, FRAPPÉ can enhance its fairness.</p>
<h3 id="2-broad-applicability">2. Broad applicability:</h3>
<p>FRAPPÉ can be used with any quantitative concept of fairness, unlike previous post-processing techniques which are tailored to particular definitions of fairness (such as statistical parity) or problematic situations (such as binary sensitive attributes).  This makes it possible to take into account parameters incompatible with previous post-processing techniques due to continuous sensitive qualities (e.g. age, income).
FRAPPÉ can enforce different notions of fairness, including:</p>
<ul>
<li><strong>Statistical Parity</strong> (ensuring similar outcomes across groups)</li>
<li><strong>Equal Opportunity</strong> (ensuring equal true positive rates across groups)</li>
<li><strong>Equalized Odds</strong> (balancing both true and false positive rates across groups)</li>
</ul>
<h3 id="3absence-of-sensitive-attributes-during-inference">3.Absence of sensitive attributes during inference:</h3>
<p>Many post-processing techniques require knowledge of sensitive attributes (such as gender or race) at the time of inference, which is sometimes impractical or morally dubious.  By simulating the post-hoc transformation as a function of all covariates, not just the sensitive characteristic, FRAPPÉ gets around this problem.</p>
<h3 id="4efficient-and-modular">4.Efficient and modular:</h3>
<p>The modular design of FRAPPÉ&rsquo; makes it easier to adapt to different definitions of fairness and speeds up calculations since only the post-hoc module needs to be modified, rather than the entire model.</p>
<h3 id="5handling-partial-group-labels">5.Handling Partial Group Labels:</h3>
<p>In most real-life scenarios, only a small part of the data has sensitive attributes. In this case FRAPPÉ outperforms in-processing methods , as it avoids over-adjusting the fairness regularizer and keeps a good trade-off between fairness and accuracy.</p>
<hr>
<h2 id="how-frappé-works">How FRAPPÉ Works</h2>
<p>Here&rsquo;s how it works:</p>
<p>Base model: Predictions are made using a pre-trained model, such as logistic regression, random forest or neural network.</p>
<p>Post hoc module: The results of the basic model go through direct additive alternations. Like processing techniques, this module is trained using a fairness regularizer, but during inference, it does not need the access to sensitive attributes or the training pipeline .</p>
<p>The concept is  <strong>applying a fairness adjustment layer after model predictions</strong>.  To decrease bias, this layer, called <code>TPP(x)</code>, changes each prediction.  The updated prediction is as follows:</p>
<p>Base_prediction + TPP(x) = Fair_prediction</p>
<ul>
<li><code>base_prediction</code> : the prediction of the first model.</li>
<li><code>TPP(x)</code> : the fairness correction based on input characteristics.</li>
</ul>
<h3 id="key-advantages">Key Advantages</h3>
<p>The authors of the article use comprehensive trials on a number of datasets, including Adult, COMPAS, HSLS, ENEM and Communities &amp; Crime, to show the effectiveness of FRAPPÉ.  The key findings are as follows:</p>
<ul>
<li>
<p>FRAPPÉ is more computationally efficient than in-house processing techniques, while offering comparable or better fairness-error trade-offs.</p>
</li>
<li>
<p>It outperforms current post-processing methods such as FairProjection, particularly in the presence of continuous sensitive features or partial group labels.</p>
</li>
<li>
<p>Due to its modular nature, FRAPPÉ is very useful for real-world applications, and can be quickly adapted to a variety of fairness definitions or trade-offs.</p>
</li>
</ul>
<h2 id="real-world-example">Real-World Example</h2>
<p>Let&rsquo;s take in mind a hiring algorithm ,it basically scores applicants based on their resumes. An exemple of historical bias means female applicants womm receive lower scores on average. Traditionally, we’d need to retrain the whole model to correct this.</p>
<p>Using FRAPPÉ, we can leave the original model untouched — we just add a <strong>fairness correction layer</strong> that adjusts scores to ensure fair treatment across genders. This is faster, cheaper, and works even if we didn’t train the original model.</p>
<h2 id="how-well-does-frappé-work">How Well Does FRAPPÉ Work?</h2>
<h3 id="as-good-as-in-processing">As Good as In-Processing</h3>
<p>Experimenting with datasets such as <strong>Adult Income</strong>, <strong>COMPAS</strong>, and <strong>HSLS</strong>, FRAPPÉ scored <strong>similar fairness-accuracy trade-offs</strong> as traditional in-processing moduls without retraining.</p>
<h3 id="better-than-other-post-processing-methods">Better Than Other Post-Processing Methods</h3>
<p>Compared to <strong>FairProjection</strong>, a leading post-processing method, FRAPPÉ consistently achieved <strong>better fairness with lower accuracy loss</strong> across datasets.</p>
<h3 id="even-works-with-partial-group-labels">Even works with Partial Group Labels</h3>
<p>When even only a small percentage of the training data includes group labels (e.g., race or gender), FRAPPÉ keeps a strong performance — a great positive point compared to traditional methods, which often overfit when group labels are sparse.</p>
<h2 id="why-frappé-matters">Why FRAPPÉ Matters</h2>
<h3 id="efficiency">Efficiency:</h3>
<p>FRAPPÉ can <strong>decrease training costs upto 90%</strong> compared to in-processing approaches, as it only learns a small <strong>layer of correction</strong> and not the entire model.</p>
<h3 id="flexibility">Flexibility:</h3>
<p>Is <strong>statistical parity</strong> required instead of <strong>equal opportunity</strong>?  Just relearn the correction layer to solve the problem.</p>
<h3 id="privacy">Privacy:</h3>
<p>For FRAPPÉ the storage or extraction of sensitive demographic data is not crucial or obigatory, as it works without group labels at the time of prediction.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>When retraining is not an option, FRAPPÉ provides a useful, adaptable and efficient method for ensuring fairness in machine learning systems.</p>
<p>FRAPPÉ can make the usage of responsible AI in practical applications by <strong>separating model learning from fairness correction</strong>.</p>
<p>For those interested in ethical AI, data science or policy-making, FRAPPÉ presents a promising new tool for developing fairer technologies faster and with fewer limitations.</p>
<h2 id="what-do-you-think-about-frappé">What do you think about FRAPPÉ?</h2>
<p>Could this framework be the key to making fairness mitigation more practical and scalable? We are curious for you to share your thoughts !</p>
<h2 id="references">References</h2>
<ul>
<li>
<p>Alexandru Țifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, Flavien Prost. <em>FRAPPÉ: A Group Fairness Framework for Post-Processing Everything</em>. ICML 2024. <a href="https://arxiv.org/abs/2312.02592">arXiv Link</a></p>
</li>
<li>
<p><a href="https://responsible-ai-datascience-ipparis.github.io/tutorial/">Responsible AI Blog Guidelines - Télécom Paris</a></p>
</li>
</ul>
<h2 id="about-the-author">About the Author</h2>
<p><strong>Arij Hajji</strong></p>
<p><em>M2 Data Science, Institut Polytechnique de Paris</em><br>
<em><a href="mailto:arij.hajji@telecom-paris.fr">arij.hajji@telecom-paris.fr</a></em></p>

      
      <div class="post-date">
        <span class="g time">March 1, 2025 </span> &#8729;
         
      </div>
      
    </section>
    
    <div id="comments">
      <script src="https://utteranc.es/client.js"
    repo=ZgotmplZ
    issue-term="pathname"
    theme=ZgotmplZ
    crossorigin="anonymous"
    async>
</script>

    </div>
    
  </div>
</main>
</body>
</html>
